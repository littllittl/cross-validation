{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from random import randrange\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat2num(x):\n",
    "    if x == 'C': x = 0\n",
    "    elif x == 'Q': x = 1\n",
    "    else: x = 2\n",
    "    return x\n",
    "\n",
    "def split4cv(data4split, k = 3):  \n",
    "    k_folds = list() # k_folds = [fold1=[[X],[y]], fold2=[[X],[y]] ...]\n",
    "    fold_size = data4split.shape[0] / k \n",
    "    X,y = list(data4split.drop('survived', axis=1).values), list(data4split['survived'].values)\n",
    "    for i in range(k): # make k_folds list \n",
    "        fold = list()\n",
    "        tmp_X = list()\n",
    "        tmp_y = list()\n",
    "        while len(tmp_X) < fold_size:\n",
    "            index = randrange(len(X))  \n",
    "            tmp_X.append(X.pop(index))\n",
    "            tmp_y.append(y.pop(index))\n",
    "        fold.append([tmp_X,tmp_y])\n",
    "        k_folds.append(fold) \n",
    "    return k_folds, k\n",
    "  \n",
    "def cross_validation(data4cv,model,j):\n",
    "    k_folds, k = split4cv(train_set)\n",
    "    X, y = [k_folds[i][0][0] for i in range(k)], [k_folds[i][0][1] for i in range(k)]\n",
    "    # calculate empirical risk:\n",
    "    scores = []\n",
    "    for i in range(k):\n",
    "        X_train = np.concatenate([X[j] for j in range(k) if j != i])\n",
    "        y_train = np.concatenate([y[j] for j in range(k) if j != i])\n",
    "        X_test = X[i]\n",
    "        y_test = np.array(y[i])\n",
    "        model.fit(X_train, y_train)\n",
    "        if j==0:\n",
    "            y_pred = model.predict(X_train)\n",
    "            empirical_risk = np.sum((y_pred - y_train)**2)\n",
    "            scores.append(empirical_risk)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "            empirical_risk = np.sum((y_pred - y_test)**2)\n",
    "            scores.append(empirical_risk)\n",
    "    if j==0: \n",
    "        print('TRAIN AVERAGE EMPIRICAL RISK = ' + str(np.mean(np.array(scores))))\n",
    "    else:\n",
    "        print('TEST AVERAGE EMPIRICAL RISK = ' + str(np.mean(np.array(scores))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reading data\n",
    "train_set = pd.read_csv('train.csv', index_col = 'PassengerId')\n",
    "test_set = pd.read_csv('test.csv', index_col = 'PassengerId')\n",
    "\n",
    "''' Preprocessing dataset '''\n",
    "#Delete some feature\n",
    "train_set = train_set.drop(['cabin', 'ticket', 'name'], axis=1)\n",
    "test_set = test_set.drop(['cabin', 'ticket', 'name'], axis=1)\n",
    "#Fill missed cells in 'Age'\n",
    "train_set['age'].fillna(train_set['age'].mean(), inplace=True)\n",
    "test_set['age'].fillna(test_set['age'].mean(), inplace=True)\n",
    "# Fill missed cells in 'Embarked'\n",
    "train_set['embarked'] = train_set['embarked'].apply(cat2num)\n",
    "test_set['embarked'] = test_set['embarked'].apply(cat2num)\n",
    "\n",
    "# Encode categorical features to numerical\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(train_set['sex'])\n",
    "train_set['sex'] = label_encoder.transform(train_set['sex'])\n",
    "\n",
    "label_encoder.fit(test_set['sex'])\n",
    "test_set['sex'] = label_encoder.transform(test_set['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning the hyperparmeters:\n",
      "\n",
      "\n",
      "RandomForestClassifier\n",
      "n_estimators: 8\n",
      "max_depth:5\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 87.33333333333333\n",
      "TEST AVERAGE EMPIRICAL RISK = 53.666666666666664\n",
      "\n",
      "\n",
      "\n",
      "n_estimators: 15\n",
      "max_depth:8\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 50.333333333333336\n",
      "TEST AVERAGE EMPIRICAL RISK = 52.666666666666664\n",
      "\n",
      "\n",
      "\n",
      "n_estimators: 30\n",
      "max_depth:10\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 29.333333333333332\n",
      "TEST AVERAGE EMPIRICAL RISK = 52.0\n",
      "\n",
      "\n",
      "\n",
      "n_estimators: 100\n",
      "max_depth:12\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 18.333333333333332\n",
      "TEST AVERAGE EMPIRICAL RISK = 53.333333333333336\n",
      "\n",
      "\n",
      "\n",
      "n_estimators: 1000\n",
      "max_depth:25\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 7.333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Tuning the hyperparmeters\n",
    "print('Tuning the hyperparmeters:\\n\\n')\n",
    "print('RandomForestClassifier') \n",
    "n_forests = [8, 15, 30, 100, 1000, 3000] # n_estimators\n",
    "n_level   = [5, 8, 10, 12, 25, 50]     # max_depth\n",
    "for i in range(len(n_level)):\n",
    "    randomforest = RandomForestClassifier(n_estimators = n_forests[i], criterion ='gini',\n",
    "                               max_features = 'auto', min_samples_split=2, max_depth=n_level[i], random_state=42, n_jobs=-1)\n",
    "    print('n_estimators: ' + str(n_forests[i]))\n",
    "    print('max_depth:' + str(n_level[i]))\n",
    "    cross_validation(train_set,randomforest,0)\n",
    "    cross_validation(test_set,randomforest,1)\n",
    "    print('\\n\\n')\n",
    "print('LogisticRegression')\n",
    "print('solver=newton-cg')\n",
    "lr=LogisticRegression(solver='newton-cg')\n",
    "cross_validation(train_set,lr,0)\n",
    "cross_validation(test_set,lr,1)\n",
    "print('\\n\\n')\n",
    "print('solver=lbfgs')\n",
    "lr=LogisticRegression(solver='lbfgs')\n",
    "cross_validation(train_set,lr,0)\n",
    "cross_validation(test_set,lr,1)\n",
    "print('\\n\\n')\n",
    "print('solver=liblinear')\n",
    "lr=LogisticRegression(solver='liblinear')\n",
    "cross_validation(train_set,lr,0)\n",
    "cross_validation(test_set,lr,1)\n",
    "print('\\n\\n')\n",
    "print('solver=sag')\n",
    "lr=LogisticRegression(solver='sag')\n",
    "cross_validation(train_set,lr,0)\n",
    "cross_validation(test_set,lr,1)\n",
    "print('\\n\\n')\n",
    "print('solver=saga')\n",
    "lr=LogisticRegression(solver='saga')\n",
    "cross_validation(train_set,lr,0)\n",
    "cross_validation(test_set,lr,1)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Наименьший эмпирический риск получается при классификаторе RandomForest с количеством деревьев 1000 \n",
    "#и количеством уровней 25, при классификаторе LogisticRegression наиболее оптимальное значение получаем при solver=newton-cg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
