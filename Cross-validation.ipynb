{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansk/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "from random import randrange, choice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat2num(x):\n",
    "    if x == 'C': x = 0\n",
    "    elif x == 'Q': x = 1\n",
    "    else: x = 2\n",
    "    return x\n",
    "\n",
    "def split4cv(data4split, k = 3):  \n",
    "    k_folds = list() # k_folds = [fold1=[[X],[y]], fold2=[[X],[y]] ...]\n",
    "    fold_size = data4split.shape[0] / k \n",
    "    X,y = list(data4split.drop('survived', axis=1).values), list(data4split['survived'].values)\n",
    "    for i in range(k): # make k_folds list \n",
    "        fold = list()\n",
    "        tmp_X = list()\n",
    "        tmp_y = list()\n",
    "        while len(tmp_X) < fold_size:\n",
    "            index = randrange(len(X))  \n",
    "            tmp_X.append(X.pop(index))\n",
    "            tmp_y.append(y.pop(index))\n",
    "        fold.append([tmp_X,tmp_y])\n",
    "        k_folds.append(fold) \n",
    "    return k_folds, k\n",
    "  \n",
    "def cross_validation(data4cv,model,j):\n",
    "    k_folds, k = split4cv(train_set)\n",
    "    X, y = [k_folds[i][0][0] for i in range(k)], [k_folds[i][0][1] for i in range(k)]\n",
    "    # calculate empirical risk:\n",
    "    scores = []\n",
    "    for i in range(k):\n",
    "        X_train = np.concatenate([X[j] for j in range(k) if j != i])\n",
    "        y_train = np.concatenate([y[j] for j in range(k) if j != i])\n",
    "        X_test = X[i]\n",
    "        y_test = np.array(y[i])\n",
    "        model.fit(X_train, y_train)\n",
    "        if j==0:\n",
    "            y_pred = model.predict(X_train)\n",
    "            empirical_risk = np.sum((y_pred - y_train)**2)\n",
    "            scores.append(empirical_risk)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "            empirical_risk = np.sum((y_pred - y_test)**2)\n",
    "            scores.append(empirical_risk)\n",
    "    if j==0: \n",
    "        print('TRAIN AVERAGE EMPIRICAL RISK = ' + str(np.mean(np.array(scores))))\n",
    "    else:\n",
    "        print('TEST AVERAGE EMPIRICAL RISK = ' + str(np.mean(np.array(scores))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reading data\n",
    "train_set = pd.read_csv('train.csv', index_col = 'PassengerId')\n",
    "test_set = pd.read_csv('test.csv', index_col = 'PassengerId')\n",
    "\n",
    "''' Preprocessing dataset '''\n",
    "#Delete some feature\n",
    "train_set = train_set.drop(['cabin', 'ticket', 'name'], axis=1)\n",
    "test_set = test_set.drop(['cabin', 'ticket', 'name'], axis=1)\n",
    "#Fill missed cells in 'Age'\n",
    "train_set['age'].fillna(train_set['age'].mean(), inplace=True)\n",
    "test_set['age'].fillna(test_set['age'].mean(), inplace=True)\n",
    "# Fill missed cells in 'Embarked'\n",
    "train_set['embarked'] = train_set['embarked'].apply(cat2num)\n",
    "test_set['embarked'] = test_set['embarked'].apply(cat2num)\n",
    "\n",
    "# Encode categorical features to numerical\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(train_set['sex'])\n",
    "train_set['sex'] = label_encoder.transform(train_set['sex'])\n",
    "\n",
    "label_encoder.fit(test_set['sex'])\n",
    "test_set['sex'] = label_encoder.transform(test_set['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning the hyperparmeters:\n",
      "\n",
      "\n",
      "RandomForestClassifier\n",
      "n_estimators: 8\n",
      "max_depth:5\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 91.33333333333333\n",
      "TEST AVERAGE EMPIRICAL RISK = 58.333333333333336\n",
      "\n",
      "\n",
      "\n",
      "n_estimators: 15\n",
      "max_depth:8\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 53.333333333333336\n",
      "TEST AVERAGE EMPIRICAL RISK = 56.666666666666664\n",
      "\n",
      "\n",
      "\n",
      "n_estimators: 30\n",
      "max_depth:10\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 28.333333333333332\n",
      "TEST AVERAGE EMPIRICAL RISK = 52.333333333333336\n",
      "\n",
      "\n",
      "\n",
      "n_estimators: 100\n",
      "max_depth:12\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 17.666666666666668\n",
      "TEST AVERAGE EMPIRICAL RISK = 56.333333333333336\n",
      "\n",
      "\n",
      "\n",
      "n_estimators: 1000\n",
      "max_depth:25\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 8.0\n",
      "TEST AVERAGE EMPIRICAL RISK = 57.666666666666664\n",
      "\n",
      "\n",
      "\n",
      "n_estimators: 3000\n",
      "max_depth:50\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 8.333333333333334\n",
      "TEST AVERAGE EMPIRICAL RISK = 55.666666666666664\n",
      "\n",
      "\n",
      "\n",
      "LogisticRegression\n",
      "solver=newton-cg\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 115.33333333333333\n",
      "TEST AVERAGE EMPIRICAL RISK = 57.666666666666664\n",
      "\n",
      "\n",
      "\n",
      "solver=lbfgs\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 118.66666666666667\n",
      "TEST AVERAGE EMPIRICAL RISK = 60.0\n",
      "\n",
      "\n",
      "\n",
      "solver=liblinear\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 118.33333333333333\n",
      "TEST AVERAGE EMPIRICAL RISK = 60.666666666666664\n",
      "\n",
      "\n",
      "\n",
      "solver=sag\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 184.33333333333334\n",
      "TEST AVERAGE EMPIRICAL RISK = 92.66666666666667\n",
      "\n",
      "\n",
      "\n",
      "solver=saga\n",
      "TRAIN AVERAGE EMPIRICAL RISK = 183.66666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST AVERAGE EMPIRICAL RISK = 93.0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tuning the hyperparmeters\n",
    "print('Tuning the hyperparmeters:\\n\\n')\n",
    "print('RandomForestClassifier') \n",
    "n_forests = [8, 15, 30, 100, 1000, 3000] # n_estimators\n",
    "n_level   = [5, 8, 10, 12, 25, 50]     # max_depth\n",
    "for i in range(len(n_level)):\n",
    "    randomforest = RandomForestClassifier(n_estimators = n_forests[i], criterion ='gini',\n",
    "                               max_features = 'auto', min_samples_split=2, max_depth=n_level[i], random_state=42, n_jobs=-1)\n",
    "    print('n_estimators: ' + str(n_forests[i]))\n",
    "    print('max_depth:' + str(n_level[i]))\n",
    "    cross_validation(train_set,randomforest,0)\n",
    "    cross_validation(test_set,randomforest,1)\n",
    "    print('\\n\\n')\n",
    "print('LogisticRegression')\n",
    "print('solver=newton-cg')\n",
    "lr=LogisticRegression(solver='newton-cg')\n",
    "cross_validation(train_set,lr,0)\n",
    "cross_validation(test_set,lr,1)\n",
    "print('\\n\\n')\n",
    "print('solver=lbfgs')\n",
    "lr=LogisticRegression(solver='lbfgs')\n",
    "cross_validation(train_set,lr,0)\n",
    "cross_validation(test_set,lr,1)\n",
    "print('\\n\\n')\n",
    "print('solver=liblinear')\n",
    "lr=LogisticRegression(solver='liblinear')\n",
    "cross_validation(train_set,lr,0)\n",
    "cross_validation(test_set,lr,1)\n",
    "print('\\n\\n')\n",
    "print('solver=sag')\n",
    "lr=LogisticRegression(solver='sag')\n",
    "cross_validation(train_set,lr,0)\n",
    "cross_validation(test_set,lr,1)\n",
    "print('\\n\\n')\n",
    "print('solver=saga')\n",
    "lr=LogisticRegression(solver='saga')\n",
    "cross_validation(train_set,lr,0)\n",
    "cross_validation(test_set,lr,1)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Наименьший эмпирический риск получается при классификаторе RandomForest с количеством деревьев 1000 \n",
    "#и количеством уровней 25, при классификаторе LogisticRegression наиболее оптимальное значение получаем при solver=newton-cg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
